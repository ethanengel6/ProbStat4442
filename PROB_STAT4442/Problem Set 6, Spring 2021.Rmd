---
title: "Problem Set 6, Spring 2021"
author: "Wendy Christensen"
output: pdf_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

# Load any packages, if any, that you use as part of your answers here
# For example: 
library(tidyverse)


```

CONTEXT: Pew Research Center data

The data in "pew_data.RData" comes from the Pew Research Center, an organization that conducts nationally-representative public opinion polls on a variety of political and social topics. Dr. Durso constructed this data set from the 2017 Pew Research Center Science and NewsSurvey, downloaded from https://www.journalism.org/datasets/2018/ on 4/16/2019. 

The variable "LIFE" contains the responses of participants to the following question:

"In general, would you say life in America today is better, worse or about the same as it was 50 years ago for people like you?"
  
1 = Better today

2 = Worse today

3 = About the same as it was 50 years ago

-1 = Refused

You will use the pew data set again for these questions, but the set of variables will be different than those used in Problem Set 5. The data for these questions is in a data set called "pew2". Please run the code chunk below before starting this problem set (you will need the tidyverse package loaded into memory before running this chunk)

```{r}

# Your working directory will need to be set to where the pew_data.RData is located on your computer

load("pew_data.RData")
pew2<-dplyr::select(dat,AGE,PPREG4,PPWORK,PPINCIMP,PPGENDER,PPETHM,IDEO,PPEDUCAT,LIFE, KNOWLEDGE,ENJOY,SNSUSE,SNSFREQ)

```


## Question 1 - 5 points

In this problem set, you will conduct a "complete case" analysis. That is, there will be no missing data in your data set at the start of your analysis. Missing values in R are denoted "NA". However, not all NAs are created equal!

Two of the new variables relate to use of social media. SNSUSE asks if the participant uses social media, and SNSFREQ asks how frequently the participant uses social media. Many of the NAs in this data set come from people who responded that they did not use social media; that is, although the responses are denoted NA, these responses are not truly missing. Therefore, such respondents should be included in the complete case analysis.  

Examine the output produced by the following chunk and answer the questions.

```{r}

attributes(pew2$SNSUSE)
table(pew2$SNSUSE, exclude = NULL) # Exclude argument allows for NAs to be displayed and counted

attributes(pew2$SNSFREQ)
table(pew2$SNSFREQ, exclude = NULL)

```

1) How many people reported not using social media?

Your answer here:

2) How many people had responses recorded as NAs for the SNSFREQ variable?

Your answer here: 

Now that you've examined the variables, recode all NAs in SNSFREQ to 6 if the participant responded "no" to the SNSUSE variable. 

```{r}

# Your code for recoding SNSFREQ



```

To verify that you recoded SNSFREQ properly, display a table showing the counts of the responses to SNSFREQ. Be sure that NAs are included in the count and that they are shown in your knitted document. Once you've done this, answer the question below.

```{r}

# Your code showing a table of counts for the recoded SNSFREQ variable

```

3) Does the number of 6's in the recoded SNSFREQ variable match the number of people who reported not using social media?

Your answer here:



## Question 2 - 10 points 

Be sure that you have completed Question 1 before starting this question, and then do the following steps *in order*:

First, count the number of observations (i.e., rows) in your data set. Once you've done so, answer the question below this code chunk.

```{r}

# Your code here to count the number of rows

```

1) How many rows are currently present in your data set?

Your answer here: 


Next, we need to identify missing values in our data set. Before writing any code to drop these variables, it helps to manually inspect your data to see what values should be considered missing. Examine your variables (including the LIFE variable) to see what responses correspond to missing values The attributes() and table() functions are useful for this, and examples of their use are shown in the previous question. Along with NAs, also consider labels such as "Not asked" and "Refused" as missing. Once you've done so, answer the three questions below this code chunk.

```{r}

# Your code for variable examination here 

```

2) How many missing values (NAs, "not asked", or "refused") are present for the recoded SNSFREQ variable? 

Your answer here:

3) How many missing values (NAs, "not asked", or "refused") are present for the AGE variable?

Your answer here:

4) How many missing values (NAs, "not asked", or "refused") are present for the IDEO variable? 

Your answer here:


Now that you know what values should be counted as missing, set these responses equal to "NA". 

```{r}

# Your code for setting all responses that are considered missing to NA here


```

Once you've set everything that's missing equal to NA, drop all rows that contain at least one NA. 

```{r}

# Your code for dropping all observations with at least one NA here


```

Finally, count the number of rows again and answer the question below the code chunk. This is the final sample size for your complete cases analysis. 

```{r}

# Your code here

```

5) How many rows are now present in your data set?

Your answer here:


One more thing: We recoded the SNSFREQ variable to have a value of 6 if SNSUSE was missing. We could do one of two things at this point. The first thing we could do (and what we will do in this case) is leave it as it is. Because we will treat SNSFREQ as a categorical variable, the value of 6 becomes just a label for a category (i.e., just another dummy vector), which we can conceptualize of as a "never" category for social media use frequency. This wouldn't work if it were a numeric variable; in such a case, we would want change the number placeholder back to NA to ensure that the arbitrary value isn't used as part of estimating a coefficient for a numeric variable. 

## Question 3 - 5 points

Be sure that you have completed all parts of Question 2 and have the results of all prior code chunks in memory before starting this question.

1) Recode the LIFE variable such that "Worse today" equals 1 and the other responses are equal to zero. 
2) Change the variables to the appropriate variable type:
   - Continuous: AGE, PPINCIMP
   - Categorical: all others

```{r}

# Your data processing code here


```

To confirm that you recoded the LIFE variable correctly, display a table showing the counts of the binarized LIFE variable

```{r}

# Your code showing a table of counts for the recoded LIFE variable here


```

1) Per your table, how many people responded "worse today" to the life question?

Your answer here: 

1) Per your table, how many people responded something other than "worse today" to the life question?

Your answer here: 


## Question 4 - 5 points

Split your data set into training, validation, and test sets. Use the following proportions: 70% training, 15% validation, and 15% test. To make your split reproducable, be sure to incorporate the random seed included in the code chunk in an appropriate place in your code.

```{r}

# Your code for splitting into training, validation, and test sets here


set.seed(123456) # makes reproducable split


```

1) How many observations are in your training set?

Your answer here: 

2) How many observations are in your validation set?

Your answer here:

3) How many observations are in your test set?

Your answer here:


## Question 5 - 5 points

Develop a set of candidate models by using forward selection to fit a series of logistic regression model using the binarization of LIFE as the outcome and all other variables in the data set as potential predictors. Use the data in the training set for this. Display each step of the forward selection by including "trace=1" as an argument in your step() function.

```{r}

# Code for your forward selection here

```

Now, save each of the models that appeared as steps in your forward selection as model objects. You will need these for the next question.

```{r}

# Code for saving each of your forward selection model steps as separate model objects

```


## Question 6 - 10 points

Using the model objects you saved in the last question, compute the deviances of these models when applied to the validation data set. (Hint: there is a good example of this in the async material in 5.2.1: backward_train_validate_test_5_2_1). 

```{r}

# Your code here computing validation set deviances


```

Once you've computed the validation deviances, display the deviances for each model. Make sure that these are visible in your knitted document. After doing this, answer the following four questions. 

```{r}

# Your code here for displaying validation set deviances


```

1) What is the validation deviance of the single-predictor model (intercept + first chosen predictor in the forward selection)?

Your answer here:

2) What is the validation deviance of the model with the most predictors (the last model fit in the forward selection)?

Your answer here:

3) Which of the models had the lowest validation deviance?

Your answer here:

4) Based on the validation deviances you computed, which model do you choose based on the results you obtained?

Your answer here: 


## Question 7 - 10 points

Please assess the performance of the model you chose in Question 6 as applied to the test data set. Please include a confusion matrix and compute accuracy, precision, recall, and F1 score for this model. Once you've constructed the confusion matrix and computed the model fit indices, answer the four question below. 

```{r}

# Your code here for the confusion matrix and computing accuracy, precision, recall, and F1 score


```

What was the model's accuracy on the test set data?

Your answer here:

What was the model's precision on the test set data?

Your answer here:

What was the model's recall on the test set data?

Your answer here:

What was the model's F1 score on the test set data?

Your answer here:
