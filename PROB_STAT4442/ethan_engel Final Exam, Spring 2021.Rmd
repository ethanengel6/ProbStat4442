---
title: "COMP 4442 Final Exam, Spring 2021"
author: "Wendy Christensen"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

library(glmnet)
library(survival)
library(survminer)
library(tidyverse)

# Load any additional packages, if any, that you use as part of your answers here

library(stringr) # For example

```

There are four questions on this midterm, all of which have multiple parts. Please be sure to provide answers to all parts of each question. Each question has an associated .csv file, which you will load into memory at the beginning of the question. All of the included data sets are simulated, so any results should not be taken as evidence for or against the existence of anything in the real world. The data were simulated to minimize the ambiguity and messiness that typifies real data. If you feel that something is ambiguous in a way that impedes your ability to answer the questions, please let me know. 

Almost there - you can do it! 

## Question 1 - Train/validate/test (30 points total)

For this question, a 400 observation data set was split such that 1/2 of the data was assigned to a training set, 1/4 was assigned to a validation set, and the remaining 1/4 was assigned to a test set. The outcome variable is y, which is contained in all three data sets, and is continuous. The predictor variables, also contained in all three data sets, are all continuous.

Run the code chunk below to load the data into memory before beginning your work on this question. If you need to change any variable types, please do so here.

```{r}

q1.train <- read.csv("q1_train.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

q1.valid <- read.csv("q1_valid.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

q1.test <- read.csv("q1_test.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

# Any variable type changes, if necessary



str(q1.train)
str(q1.valid)
str(q1.test)

```

# Q1, Part 1 - Generating candidate models on the training set (15 points)

You will generate four candidate models: a model obtained through forward selection, a model obtained through backward selection, a cross-validated ridge regression model, and a cross-validated lasso regression model. For both cross-validated models, use the model associated with lambda.1se. You will fit all of these models to the training data (q1.train), using y as the outcome and x1-x9 as the predictors. Please do not include interactions or squares in your pool of potential predictors. Once you have done this, answer the four questions below.

Forward selection: 
```{r}

# Code for your forward selection here; please include trace=1 in your function
forward_vars<-as.formula("y ~ x1+x2+x3+x4+x5+x6+x7+x8+x9")
q1_forward<-step(lm(y~1,data=q1.train),scope=forward_vars,
direction="forward",trace=1)


# Display the final model selected by forward selection here
summary(q1_forward)

```

Backward selection: 
```{r}

# Code for your backward selection here; please include trace=1 in your function
min.model<-as.formula("y~1")

q1_backward<-step(lm(forward_vars,data=q1.train),scope=min.model,
direction="backward",trace=1)

# Display the final model selected by backward selection here
summary(q1_backward)

```

Cross-validated ridge regression (lambda.1se); please use the provided random seed
```{r}

set.seed(123456)

# Code for your cross validated ridge regression here
X<-data.matrix(dplyr::select(q1.train, -y))
Y<-q1.train$y

q1.ridge<-cv.glmnet(x=X,y=Y,alpha=0)


# Display the coefficients associated with lambda.1se ridge model below
q1.ridge$lambda.1se
coef(q1.ridge, s = "lambda.1se")

```

Cross-validated lasso regression (lambda.1se); please use the provided random seed
```{r}

set.seed(123456)

# Code for your lasso regression here
q1.lasso<-cv.glmnet(x=X,y=Y)

# Display the coefficients associated with lambda.1se lasso model below
q1.lasso$lambda.1se
coef(q1.lasso, s = "lambda.1se")

```

1) What predictors are included in the model selected by forward selection?

Your answer here: The four predictors in the model produced by forward selection were: x2, x6, x7 & x9.

2) What predictors are included in the model selected by backward selection?

Your answer here: The four predictors in the model produced by backward selection were: x2, x6, x7 & x9, the same as for forward selection.

3) What predictors are included in the cross-validated ridge model (lambda.1se)?

Your answer here:  All nine of the predictor variables, x1 through x9, are included in the lambda.1se cross validated ridge model.

4) What predictors are included in the cross-validated lasso model (lambda.1se)?

Your answer here: Three predictors, x2, x6 & x7, ended up in the lambda.1se cross validated lasso model.



# Q1, Part 2 - Fitting candidate models on the validation set and choosing a model (10 points)

Now, use the validation data (q1.valid) to generate new predictions for the four candidate models obtained in the previous part. Use these to compute the mean squared prediction error to evaluate how well each candidate model (which was estimated using training data) predicts the y's in the validation data set. Once you have done this, answer the three questions below.

Forward model:
```{r}

# Obtain new predictions using the validation set
X2<-data.matrix(dplyr::select(q1.valid, -y))
Y2<-q1.valid$y

forward.pred <- predict(q1_forward, newx = X2)


# Compute mean squared prediction error for the forward model
forward.mse<-mean((forward.pred-Y2)^2)
forward.mse

```

Backward model:
```{r}

# Obtain new predictions using the validation set
backward.pred <- predict(q1_backward, newx = X2)


# Compute mean squared prediction error for the backward model
backward.mse<-mean((backward.pred-Y2)^2)
backward.mse


```

Cross-validated ridge model (lambda.1se):
```{r}

# Obtain new predictions using the validation set
ridge.pred <- predict(q1.ridge, s = q1.ridge$lambda.1se, newx = X2)


# Compute mean squared prediction error for the cross-validated ridge model
ridge.mse<-mean((ridge.pred-Y2)^2)
ridge.mse

```

Cross-validated lasso model (lambda.1se):
```{r}

# Obtain new predictions using the validation set
lasso.pred <- predict(q1.lasso, s = q1.lasso$lambda.1se, newx = X2)

# Compute mean squared prediction error for the cross-validated lasso model
lasso.mse<-mean((lasso.pred-Y2)^2)
lasso.mse

```

1) List the mean squared prediction error for each of your candidate models here:

Forward model mean squared prediction error: 54.53578
Backward model mean squared prediction error: 54.53578
Ridge model mean squared prediction error: 26.1934
Lasso model mean squared prediction error: 27.47134

2) Based on the mean squared prediction error, which model do you choose? 

Your answer here: If mean squared prediction error is the criterion to be focused upon, then the cross validated ridge model would be the one selected.

3) Display the coefficient values for the model you chose here:
(Intercept)  4.151891889
X            0.002050961
x1           0.270386390
x2           0.841492644
x3           0.134187770
x4          -0.145202039
x5           0.190372069
x6           0.504532748
x7           1.249197142
x8           0.434017456
x9           0.511359507

```{r}

# Display candidate model chosen based on performance on the validation set here
q1.ridge$lambda.1se
coef(q1.ridge, s = "lambda.1se")



```


# Q1, Part 3 - Testing the final model on the test set (5 points)

Now that you've chosen a model, you will test this model's generalizability by generating new predictions for the final model obtained in the previous part. Use these new predictions to compute the mean squared prediction error to evaluate how well the final model (which was estimated using training data) predicts the y's in the test data set. Once you've done this, answer the question below:

```{r}

# Obtain new predictions using the test set
X3<-data.matrix(dplyr::select(q1.test, -y))
Y3<-q1.test$y
ridge.test.pred <- predict(q1.ridge, s = q1.ridge$lambda.1se, newx = X3)

# Compute mean squared prediction error for the final model
ridge.test.mse<-mean((ridge.test.pred-Y3)^2)
ridge.test.mse

```

1) What is the mean squared prediction error for the final model making predictions on the test set?

Your answer here: 34.59765 




## Question 2 - Evaluating the generalizability of a "shrunk" logistic regression model (30 points total)

For this question, a different 400-observation data set was split such that 2/3 of it was assigned to a training set and the remaining 1/3 was assigned to a test set. The outcome variable is y, which is binary. The predictor variables, V1 - V10, are all continuous. 

Run the code chunk below to load the data into memory before beginning your work on this question. If you need to change any variable types, please do so here.

```{r}

log.train <- read.csv("logistic_train.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

log.test <- read.csv("logistic_test.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

# Any variable type changes, if necessary


str(log.train)
str(log.test)

```

# Q2, Part 1 - Fitting the "shrunk" logistic model (5 points)

I fitted a penalized logistic regression model to the training data; that is, I applied a method of coefficient shrinkage to a logistic regression model fitted to the training. Run the code chunk below to fit this model and answer the two questions below.

```{r}

x.train.mat<-model.matrix(y~.,data=log.train)
x.train.matrix<-x.train.mat[,-1] 

y.train.vec<-log.train$y

set.seed(12345)

cvfit.log = cv.glmnet(x=x.train.matrix, y=y.train.vec, family="binomial", alpha=1)  
plot(cvfit.log)
coef(cvfit.log, s = "lambda.min")

```

1) What type of penalized/"shrunk" logistic regression model did I fit to the training data?

Your answer here: From the alpha value of 1 and the fact that predictor variables dropped out, it can be deduced that a lasso logistic regression was performed.

2) Which predictors (not including the intercept) are in the lambda.min penalized/"shrunk" logistic regression model? 

Your answer here (list the included predictors): The remaining predictors are v1, v4, v7, v8 & v10.


# Q2, Part 2 - Obtaining new predictions from test data (5 points)

In this section, you will generate new predictions of Y based on the values of the predictors in the test set. That is, you will use the predictor values from log.test to predict new y values. Once you have done this, you will then save your predicted y values as a .csv file, which you will submit with your final exam submission.

```{r}

# Your code here
x.test.mat<-model.matrix(y~.,data=log.test)
x.test.matrix<-x.test.mat[,-1] 

log.probs<-predict(cvfit.log,x.test.matrix,type="response")
log.preds<-ifelse(log.probs>0.5,1,0)


# Don't forget to export your predicted y values using write.csv()
write.csv(log.preds,"C:\\Users\\elain\\Desktop\\Final_Exam_SP21_student\\logpreds.csv")

```

Next, display the counts of both the predicted y values and the actual y values in log.test. Remember that these are binary variables. Hint: this will help you check your work in the next part. 

```{r}

# Counts for predicted y's
table(log.preds)


# Counts for actual y's
table(log.test$y)


```


# Q2, Part 3 - Evaluating the generalizability on the test set (20 points)

For this part, you will create a confusion matrix and compute the four model indices we discussed in class: accuracy, precision, recall, and F1 score. You may do this manually or use a function to compute these for you, but you *must* display the confusion matrix and how you obtained the indices in your knitted document for full credit. Write your code for this below and answer the questions below.

```{r}

## Your code for creating and displaying the confusion matrix
log.confmat<-table(log.preds,log.test$y)
log.confmat



## Your code for computing and displaying the model indices 
logmodel.accuracy<-(log.confmat[1,1]+log.confmat[2,2])/(log.confmat[1,1]+log.confmat[1,2]+log.confmat[2,1]+log.confmat[2,2])
logmodel.accuracy
logmodel.precision<-log.confmat[2,2]/(log.confmat[2,1]+log.confmat[2,2])
logmodel.precision
logmodel.recall<-log.confmat[2,2]/(log.confmat[1,2]+log.confmat[2,2])
logmodel.recall
logmodel.f1<-2*(logmodel.precision*logmodel.recall)/(logmodel.precision+logmodel.recall)
logmodel.f1



```

1) Fill in the values of your confusion matrix. Note that you should also have the confusion matrix displayed as part of the output from your code. 

    True positives (your answer here): 42
    False positives (your answer here):23 
    True negatives (your answer here): 24
    False negatives (your answer here): 11

2) Fill in the values of your model indices. Again, be sure that you also have these displayed as part of the output from your code. 

    Accuracy: .66
    Precision:.646
    Recall: .79
    F1 score: .71



## Question 3 - Modeling count data (20 points total)

Alyssa analyzed workplace injury incidents obtained from a shipping warehouse. She sampled 45 employee records at random and recorded the number of workplace injury reports in the employee's file in the past five years ("reports") and how many years since the employee underwent safety training certification ("years"). 

Run the code chunk below to load the data into memory before beginning your work on this question. If you need to change any variable types, please do so here.

```{r}

injury <- read.csv("injury.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

# Any variable type changes, if necessary

str(injury)

```

# Q3, Part 1 - Describing the data (10 points)

Before starting the analysis, Alyssa conducted some descriptive analyses to present alongside her modeling work. Please recreate the descriptive analyses she conducted by answering the following three questions. Any code you use to answer the questions should go into the chunk below:

```{r}

# Code for your descriptive analyses here
table(injury$reports<=1)/45*100
23/45*100

median(injury$years[injury$reports>=6])

median(injury$reports[injury$years<5])

```

1) What percentage of the sampled employees had zero or one injury report in their files? 

Your answer here: 51.11111

2) Among workers who had 6 or more injury reports, what was the median number of years since their most recent safety certification?

Your answer here: 8.5

3) Among workers who had a safety certification that was less than 5 years old, what was the median number of injury reports?

Your answer here: 1


# Q3, Part 2 - Fitting a quasipoisson model (10 points)

After some exploratory modeling, Alyssa determined that a quasipoisson model was the most appropriate model for these data. 

First, please re-create her analysis by fitting a quasipoisson model to the data, using reports as the outcome and years as the predictor. 

```{r}

# Code for your model here
injury.qp<-glm(reports~years,data=injury,family="quasipoisson")
summary(injury.qp)


```

Next, answer the following three questions about the analysis:

1) Was the number of years since safety training a significant predictor of the number of injury reports?

Your answer here: Yes, the t value of 5.168 and corresponding p value of 5.83e-6 mean the years since safety training were an extremely significant predictor of injury report count.

2) Based on the results, does the predicted count of workplace injury reports *increase* or *decrease* as the number of years since an employees most recent safety certification increases? How do you know? 

Your answer here: Since the predictor coefficient is a positive value (.233), the predicted count of workplace injury reports increases as the number of years since most recent safety certification increases.

3) Based on your answer to the previous question, do you think Alyssa recommended greater frequency of safety certification or a lower frequency of safety certification?

Your answer here: Based on the positive predictor coefficient for number of years since most recent safety certification, Alyssa would recommend a greater frequency of safety certifications for the employees.





## Question 4 - Modeling event data (20 points total)

In a certain community mental health clinic, a prospective client must first submit a completed intake form; upon approval, the client may then schedule a first visit with a provider. Ahmed designed an intervention study to determine if different follow-up schedules ("group") decrease the amount of time in between the client's approval and the client's first appointment ("first_visit", in decimal days). Clients in Group A, which was the current standard for client follow-up at the time, received a reminder via text message to schedule their first appointment 14 days after approval. Clients in Group B received reminders 14 and 21 days after approval, and clients in Group C received reminders 7 and 14 days after approval. If the client saw a provider within 30 days of approval, "status" is equal to 1; otherwise, it's equal to zero. 

Run the code chunk below to load the data into memory before beginning your work on this question. If you need to change any variable types, please do so here.

```{r}

intake <- read.csv("enrollment.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

# Any variable type changes, if necessary

str(intake)

```

# Q4, Part 1 - Kaplan-Meier curves and censoring (10 points)

Ahmed started his analysis by fitting Kaplan-Meier curves for each group, which can be seen by running the code chunk below. Please examine this visualization and answer the three questions below. 

```{r}

fit <- survfit(Surv(first_visit,status) ~ group,
               data = intake)
ggsurvplot(fit)

```

1) Based on what you see in the Kaplan-Meier curves, which group had the most censored observations?

Your answer here: Group A had the most censored observations, as its curve had the highest remaining "survival" portion at the end of the 30 day observation period.

2) The line for group C ends somewhere between 20 and 30 days. What does this mean for members of group C in terms of their time-to-first visit?

Your answer here: This means that every member of Group C had been seen between 20-30 days.  (It looks like around 26 days)

3) During the design phase, Ahmed took steps to prevent left censoring from occurring. If this had happened and a client's status was left-censored, what would that have meant with regard to their time-to-first visit? 

Your answer here: Left-censoring in this case means that a client had been seen prior to the observation window beginning, prior to official approval, it would seem.


# Q4, Part 2: Fitting parametric survival models (10 points)

Next, Ahmed fitted two accelerated failure time (AFT) models: an exponential AFT model and a Weibull AFT model. Run the code chunk below and review the results of the two analyses.

```{r}

model.weib<-survreg(Surv(first_visit,status)~group, dist="weibull",data=intake)
summary(model.weib)


model.exp<-survreg(Surv(first_visit,status)~group, dist="exponential",data=intake)
summary(model.exp)

```

He then created a Kaplan-Meier plot with overlaid cumulative distribution curves for both models. The orange cumulative distribution curves are derived from the fitted Weibull AFT model, and the blue curves are from the fitted exponential AFT model. Run the code chunk below to view these and answer the four questions below. 

```{r}

plot(survfit(Surv(first_visit,status) ~ group,
               data = intake),lty=c(1,3,5),xlab="Survival Probability")
legend("bottomleft", c("a", "b","c"), lty = c(1,3,5))
points(seq(0,30,by=.2),
       1-psurvreg(seq(0,30,by=.2),mean=model.weib$coefficients[1],
                  scale=model.weib$scale),type="l",lty=1,col="orange")
points(seq(0,30,by=.2),
       1-pexp(seq(0,30,by=.2),rate=1/exp(model.exp$coefficients[1])),
       type="l", lty=1,col="blue")
points(seq(0,30,by=.2),
       1-psurvreg(seq(0,30,by=.2),mean=sum(model.weib$coefficients[c(1,2)]),
                  scale=model.weib$scale),type="l",lty=3,col="orange")
points(seq(0,30,by=.2),
       1-psurvreg(seq(0,30,by=.2),mean=sum(model.exp$coefficients[c(1,2)]),
                  distribution = "exponential"),type="l",lty=3,col="blue")
points(seq(0,30,by=.2),
      1-psurvreg(seq(0,30,by=.2),mean=sum(model.weib$coefficients[c(1,3)]),scale=model.weib$scale),
       type="l",lty=5,col="orange")
points(seq(0,30,by=.2),
       1-psurvreg(seq(0,30,by=.2),mean=sum(model.exp$coefficients[c(1,3)]),
                  distribution = "exponential"),type="l",lty=5,col="blue")

```

1) Based on the visualization, which of the two models - exponential AFT (blue) or Weibull AFT (orange) - appear to best model the time-to-first visit for the different intervention groups?

Your answer here: The Weibull AFT curves are very closely modeling the time-to-first visit plots, much better than the exponential AFTs. 

2) Look at the model output for the model you chose in the previous question. Is there evidence of a difference between Group A and Group B?

Your answer here: WIth a z value of -3.21 and corresponding p value of 0.0013, there is compelling statistical evidence of a difference in time to first visit between groups A & B.

3) From this same model output, is there evidence of a difference between Group A and Group C?

Your answer here: With a z value of -8.52 and p value <2e-16, there is overwhelming statistical evidence of a difference in time to first visit between groups A & C.

4) Considering the results shown in both parts 1 and 2 of this question, which follow-up schedule - Day 14 only, Days 7 and 14, or Days 14 and 21 - do you think Ahmed recommended to the clinic? Please note that stating the group letter alone here won't earn full credit; you must explicitly reference the specific intervention the group experienced. 

Your answer here: The intervention for Group C, texting appointment reminders at days 7 and 14 after initial approval, would be the most likely recommendation that Ahmed would make.  


## All done - you did it! Treat yourself to something nice :)