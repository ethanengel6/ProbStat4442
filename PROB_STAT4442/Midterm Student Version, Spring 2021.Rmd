---
title: "COMP 4442 Midterm, Spring 2021"
author: "Wendy Christensen"
output: pdf_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(leaps)
library(onewaytests)
# Load any additional packages, if any, that you use as part of your answers here



```

There are six questions on this midterm, all of which have multiple parts. Please be sure to provide answers to all parts of each question. Each question has an associated .csv file, which you will load into memory at the beginning of the question. All of the included data sets are simulated, so any results should not be taken as evidence for or against the existence of anything in the real world. The data were simulated to minimize the ambiguity and messiness that typifies real data. If you feel that something is ambiguous in a way that impedes your ability to answer the questions, please let me know. 

I believe in you! 

## Question 1: Basic ANOVA - 10 points total

A tire manufacturing company wants to know if different formulations of tire rubber result in differences in tire durability. They are interested in four different rubber formulations ("form"). To test this, 20 tires of each rubber formulation are selected for testing. Aside from the rubber formulation, all 80 tires in this experiment are otherwise exactly the same. The durability of each tire is tested using a durability machine, which mimics the forces and stress a tire is exposed to when installed in a standard sedan driving down a flat asphalt road at 60 miles per hour. The machines tracks how many miles the tire has "traveled" based on the number of rotations of the tire. The durability test stops when the tire's structure fails, which is when the durability machine records the number of traveled miles ("miles"). The data from this hypothetical experiment is contained in the Q1data.csv file. 

Run the code chunk below to load the data into memory before beginning your work on this question. If you need to change any variable types, please do so here.
```{r }

tires <- read.csv("Q1data.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

# Any variable type changes, if necessary
tires$form_factor <- factor(tires$form)

str(tires)

```

# Q1, Part 1: Assessing the normality of groups assumption (2 points)

You will assess the assumption of normality in two ways: quantitatively and visually.

In this first code chunk, please conduct an appropriate quantitative assessment of the normality assumption and display the results.
  
```{r }

# Code for your quantitative assessment of the normality assumption goes here
with(tires, tapply(miles, form_factor, shapiro.test))


```
None of the four tire form factor levels quantitatively come close to violating the normality assumption.  The lowest p-value generated by the Shapiro-Wilk normality test for the four tire form factor levels was form b at .31, nowhere near a problematic value. 

In this second code chunk, please conduct an appropriate visual assessment of the normality assumption and display the visualization/s you create. 

```{r }

# Code for your visual assessment of the normality assumption goes here
ggplot(tires, aes(sample=miles))+stat_qq()+stat_qq_line()+facet_wrap( ~ form_factor)


```
All four qqplots generated for the tire form factor levels are linear enough to assume no violations of normality of data, consistent with the quantitative assessments.

# Q1, Part 2: Assessing the equality of variances of groups assumption (2 points)

You will assess the assumption of equality of variances in two ways: quantitatively and visually.

In this first code chunk, please conduct an appropriate quantitative assessment of the equality of variances assumption and display the results.
  
```{r }

# Code for your quantitative assessment of the equality of variances assumption goes here
bf.test(miles ~ form_factor, data = tires)


```
The output of the Brown-Forsythe test is fairly straight forward.  A high p-value (.79) mean that the differences in the sample variances are not statistically significant.


In this second code chunk, please conduct an appropriate visual assessment of the equality of variances assumption and display the visualization/s you create. 

```{r }

# Code for your visual assessment of the equality of variances assumption goes here
ggplot(dat=tires,aes(x=form_factor,y=miles))+geom_boxplot()+geom_point()


```
The boxplots generated reveal that all four of the tire form factors have similar ranges and IQRS, as well as an absence of outliers.  This is consistent with the previous quantitative assessment of equality of variances assumption.  

# Q1, Part 3: Fitting the ANOVA model (2 points)

Now, you will conduct an ANOVA on the tires data set that can provide an answer to the research question: do different formulations of tire rubber have different durability? Please be sure to display the results of your analysis. 

```{r }

# Code for your ANOVA goes here
tires.anova<-aov(miles~form_factor,data=tires)

# Don't forget to display the results using the summary() function!
summary(tires.anova)
```

# Q1, Part 4: Interpreting the ANOVA results (2 points)

1) What is the null hypothesis being tested by the ANOVA you conducted? Based on the results of your analysis, do you reject or fail to reject this null hypothesis?

Your answer here: The null hypothesis for this particular ANOVA test, is that there is no meaningful difference in tire durability, measured in simulated miles driven, between the four different formulations of tire rubber.  With such a low F-statistic(.345) and high p-value (.793), the null hypothesis would not be rejected.  

2) What do the results of your ANOVA suggest about the research question? That is, what is your answer to the tire manufacturer's research question about tire durability? 

Your answer here: At this point, there is no statistical evidence to suggest any difference in tire durability between the different formulations of tire rubber.


## Question 2: Multifactor ANOVA - 10 points 

A health researcher designed an experiment to test the effects of two medications, Lowesterol and Lipidown, on LDL cholesterol levels of people who had been diagnosed as having high cholesterol but no other health problems. He recruited 160 participants, all of whom took two pills each day for 90 days. For 40 participants, both pills were placebos. For 40 participants, one pill contained Lowesterol and the other pill was a placebo. For 40 participants, one pill contained Lipidown and the other pill was a placebo. For the last 40 participants, one pill contained Lowesterol and the other contained Lipidown. After 90 days, each participant gave a blood sample and the LDL level in their blood was recorded. The data from this hypothetical experiment is contained in the Q2data.csv file. 

Run the code chunk below to load the data into memory before beginning your work on this question.
```{r }

drugs <- read.csv("Q2data.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

# Any variable type changes, if necessary
drugs$lowesterol_factor <- factor(drugs$lowesterol)
drugs$lipidown_factor <- factor(drugs$lipidown)
str(drugs)

```

# Q2, Part 1: Fitting the factorial ANOVA model (4 points)

Now, you will conduct a two-way ANOVA with an interaction on the drug data. Use post.ldl as the outcome. Please be sure to display the results of your analysis. 

```{r }

# Code for your ANOVA goes here
drugs.model<-aov(post.ldl~lowesterol_factor*lipidown_factor,data=drugs)


# Don't forget to display the results using the summary() function!
summary(drugs.model)
```

# Q2, Part 2: Interpreting the factorial ANOVA model (6 points)

Please state the three null hypotheses being tested in your analysis, state whether you reject or fail to reject that null hypothesis, and interpret the result in the context of the research question. After interpreting the interaction, please note if the result of the interaction test changes how you view the results of the tests of main effects. 

Main effect of Lowesterol (your answer here): There is no difference in the mean ldl cholesterol between people who take Lowesterol and people who take a placebo.  With a relatively low F-statistic of 2.442 and a p value of .12, the null hypothesis would not be rejected at the alpha=.05 level.  There is not compelling statistical evidence that taking Lowesterol is associated with lower levels of ldl cholesterol.


Main effect of Lipidown (your answer here): There is no difference in the mean ldl cholesterol between people who take Lipidown and people who take a placebo.  With a high F-statistic(>243) and extremely low p-value(<2e-16), the null hypothesis would be rejected at the alpha=.05, .01, .001 or even lower levels.  There is overwhelming statistical evidence that taking Lipidown is associated with lower levels of ldl cholesterol.


Interaction (your answer here): There is no evidence of an interaction between Lowesterol and Lipidown regarding levels of ldl cholesterol.
With a low F-statistic(1.6) and a p-value of >.2, the null hypothesis would not be rejected.  There is not evidence of an interaction between the two medications.


## Question 3: Multiple Regression - 20 points total

A security firm contracted by a shopping center wants to examine the factors that contribute to "loss" (theft of money or goods by customers or employees of a store) in the 200 stores in the shopping center. They have four pieces of information reported by the shopping center about each store: the amount of loss in dollars ("loss", continuous), the area of the store in square feet ("area", continuous), the average number of people who walk into the store on a weekly basis ("traffic", continuous), and whether the store is primarily a retail store (retail=1) or a service-oriented store (retail=0). The data from this hypothetical study is contained in the Q3data.csv file.

Run the code chunk below to load the data into memory before beginning your work on this question.
```{r }

mall <- read.csv("Q3data.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

# Any variable type changes, if necessary

str(mall)

```

# Q3, Part 1: Fitting the regression model (2 points)

Now, you will conduct a multiple regression analysis. The outcome for this regression will be loss, and the predictors will be retail, area, and traffic. Be sure to display the results of your analysis. 

```{r }

# Code for your regression goes here
loss_model<-lm(loss ~retail+area+traffic, data=mall)

# Don't forget to display the results using the summary() function!

summary(loss_model)
```


# Q3, Part 2: Checking diagnostic plots (4 points)

Please display the diagnostic plots for the model you fit in the previous part of this question and answer the question below: 

```{r }

# Code for your regression diagnostic plots
plot(loss_model)
```

1) What is the most obvious problem that all of the diagnostic plots for this model share? 

Your answer here: Observation #200 is disproportionately influencing the model.  First, it is far off of the line on the Normal QQ plot.  Secondly, it is far beyond the Cook's Distance threshold.  Finally, it is bending the linearity of the residual plots in such a manner that it is difficult to extract information about their variation.

2) What would be a good solution to this problem?

Your answer here: It would probably be best to remove the observation.  It is probably an erroneous entry and is confounding the results of the model.

# Q3, Part 3: Re-fitting the regression model (4 points)

Now, implement the solution you proposed in the last part and re-fit the regression model. Be sure to display the results of your updated analysis.

```{r }

# Code for any changes you make to implement your solution here
mall2<-mall[-c(200),]
mall2
# Code for your re-fitted regression goes here
loss_model2<-lm(loss~ retail + area + traffic, data=mall2)


# Don't forget to display the results of your analysis using the summary() function!
summary(loss_model2)
```

Next, display the updated diagnostic plots and answer the question below

```{r }

# Display your updated diagnostic plots here
plot(loss_model2)

```

Did your solution to the problem you identified in Q3, Part 2 noticeably improve the diagnostic plots of the model?

Your answer here: The diagnostic plots were dramatically improved after the removal of observation #200.  Both of the residual plots now reveal uniform variation across a horizontal.  The data lines up nicely on the QQ plot, and there are no Cook's Distance violations on the Residuals vs. Leverage plot.


# Q3, Part 4: Interpreting the re-fitted regression model (10 points)

1) Interpret the estimated intercept in the context of the predicted outcome and the predictors.

Your answer here:  Holding the area and traffic variables=0, in a service-oriented store, the predicted loss would be $3.60.

2) Interpret the coefficient associated with retail:

Your answer here:  Holding the area and traffic variables constant, a retail-store would have a predicted loss of $4.95 more than the intercept, $4.95+$3.60= $8.55.

3) What is the predicted amount of loss for a non-retail store that has an area of 1000 square feet and average weekly traffic of 200?
3.60+0*4.95+1000*1+200*.5 = $1103.60


## Question 4: Automated model selection - 35 points total

The data set Q4data.csv contains nine variables: y, x1, x2, x3, x4, x5, x6, x7, and x8. All of these variables are continuous. 

Run the code chunk below to load the data into memory before beginning your work on this question.
```{r }

many.var <- read.csv("Q4data.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

# Any variable type changes, if necessary


str(many.var)

```

# Q4, Part 1: Forward selection - 10 points

First, you will use forward selection to select a model. The outcome will be y and the pool of potential predictors will include x1, x2, x3, x4, x5, x6, x7, and x8. Be sure to include trace=1 or trace=TRUE as part of your use of the function. After this, display the model selected using forward selection. 

```{r }

# Code for your forward selection goes here
var_scope<-as.formula("y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8")
var_forward<-step(lm(y~1,data=many.var),scope=var_scope,
direction="forward",trace=1)

```

The model selected by forward selection: y ~ x7 + x2 + x4 + x1 + x3 + x5

```{r }

# Display the model selected using forward selection here!
summary(var_forward)

```


# Q4, Part 2: Backward selection - 10 points

Next, you will use backward selection to select a model. The outcome will be y and the pool of potential predictors will include x1, x2, x3, x4, x5, x6, x7, and x8. Be sure to include trace=1 or trace=TRUE as part of your use of the function. After this, display the model selected using backward selection. 

```{r }

# Code for your backward selection goes here
var.min<-as.formula("y~1")

var_backward<-step(lm(var_scope,data=many.var),scope=var.min,
direction="backward",trace=1)

```

The model selected by backward selection:y ~ x1 + x2 + x3 + x4 + x5 + x7

```{r }

# Display the model selected using backward selection here!
summary(var_backward)

```

# Q4, Part 3: Best subsets selection - 10 points

Finally, you will use best subsets selection to select a model. The outcome will be y and the pool of potential predictors will include x1, x2, x3, x4, x5, x6, x7, and x8. Be sure to display a table that has shows which predictors were selected as best for models of each size (hint: regsubsets() automatically makes this table for you, so you just need to display/print this table). After this, display the model selected using best subsets selection. 

```{r }

# Code for your best subsets selection goes here
vars_matrix<-model.matrix(as.formula("y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8"),many.var)
vars.y<-many.var$y
var_best<-regsubsets(x=vars_matrix[,2:ncol(vars_matrix)],y=vars.y,method="exhaustive",nbest=1)

```

The model selected by best subsets selection:y ~ x1 + x2 + x3 + x4 + x5 + x7

```{r }

# Display the model selected using best subsets selection here!
summary(var_best)
BICs<-summary(var_best)$bic
qs<-1:length(BICs)+2
n<-nrow(many.var)
AICs<-BICs-log(n)*qs+2*qs
AICs

```

# Q4, Part 4: Comparing models - 5 points

Now that you have conducted three methods of automated model selection on the same data set, please compare the models that were selected by each method. If the methods included different predictors in their final models, list those predictors that were different. If the methods included the same set of predictors in their final models, note that.

Please write your answers below:

Forward model vs backward model - All three of the selection methods resulted in the inclusion of the same predictor variables in the model: x1, x2, x3, x4, x5 & x7.

Forward vs best subsets model - All three of the selection methods resulted in the inclusion of the same predictor variables in the model: x1, x2, x3, x4, x5 & x7

Backward vs best subsets model - All three of the selection methods resulted in the inclusion of the same predictor variables in the model: x1, x2, x3, x4, x5 & x7


## Question 5: Nested model selection - 15 points total 

The data set Q5data.csv contains nine variables: y, x1, x2, x3, x4, x5, x6, x7, and x8. All of these variables are continuous. This is the same data set used in Question 4, but please reload the data set to ensure no "cross-contamination" between questions. 

Run the code chunk below to load the data into memory before beginning your work on this question.
```{r }

Q5.var <- read.csv("Q5data.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

# Any variable type changes, if necessary


str(Q5.var)

```

# Q5, Part 1: Identifying nested models - 10 points

I fitted five regression models using different sets of predictors. Run the code chunk below to estimate and view the models I fitted. Review the output for these models and answer the questions below.

```{r }

model.1 = lm(y~x1, data=Q5.var)
model.2 = lm(y~x1+x2,data=Q5.var)
model.3 = lm(y~x1+x3, data=Q5.var)
model.4 = lm(y~x1+x2+x3, data=Q5.var)
model.5 = lm(y~x1+x2+x3+x1:x2+x1:x3+x2:x3+x1:x2:x3, data=Q5.var)

summary(model.1)
summary(model.2)
summary(model.3)
summary(model.4)
summary(model.5)

```

1) If Model 5 (model.5) is considered to be the "full model", which of the remaining models - Models 1, 2, 3, and 4 - are nested relative to it? 


2) If Model 4 (model.4) is considered to be the "full model", which of the remaining models - Models 1, 2, and 3 - are nested relative to it? 


3) If Model 3 (model.3) is considered to be the "full model", which of the remaining models - Models 1 and 2 - are nested relative to it? 

4) In the code chunk below, specify a new model that is nested relative to Model 5 AND in which Model 2 is nested. That is, specify a model that fits the nested model relationship depicted below:

Model 5 (7 predictor coefficients) <-- (Your model, 3-6 predictor coefficients) <-- Model 2 (2 predictor coefficients)

Please note that you cannot chose any of the models already fitted in this question. You must specify a model that hasn't yet been fitted. 

```{r }

model.new <- # Your nested model here
summary(model.new)

```

# Q5, Part 2: Nested model testing - 5 points

For this part, you will conduct two nested model tests. In the first test, you will test Model 2 and the new model you specified. In the second test, you will test the new model you specified and Model 5. After you've done this, answer the question below. 

```{r }

# Your code for the nested model test between model.2 and model.new

```

```{r }

# Your code for the nested model test between model.new and model.5

```

1) Based on the result of the test between Model 2 and your new model, which model would you choose?

Your answer here:

2) Based on the result of the test between your new model and Model 5, which model would you choose? 

Your answer here: 


## Question 6: Basic logistic regression - 10 points total

A state public health agency wants to investigate the presence of dangerous amounts of lead in drinking water across households within the state. Investigators collected tap water samples from 150 single-family homes and obtained information about each house. Based on advice from an environmental agency, the investigators classified a tap water sample as being safe if it had levels below 15 parts per billion (0) or potentially dangerous if it had levels equal to or greater than 15 parts per billion (1). In addition, they tested the "hardness" (i.e, presence of dissolved calcium, magnesium, and other minerals) of the water sample, which they categorized as being low (0) or high (1). They also noted the age of the house in years and the location type of the house (urban, suburban, or rural). The data from this hypothetical study is contained in the Q6data.csv file. 

Run the code chunk below to load the data into memory before beginning your work on this question
```{r }

lead <- read.csv("Q6data.csv", header=TRUE, sep=",") # Loads the CSV file into memory. You may need to adapt this line to work on your computer

str(lead)

```

# Q6, Part 1: Fitting a logistic model - 5 points

Fit a logistic regression model using "danger" (categorical) as the outcome and "age" (continuous), "loc" (categorical), and "hard" (categorical) as predictors. Be sure to display the results of the analysis.

```{r }

# Code for your analysis here



# Don't forget to use the summary() function to display your results!

```

# Q6, Part 2: Interpreting a logistic model - 5 points

1) Based on the results of your analyses, which predictor coefficients were significantly different from zero? There is at least one. 

Your answer here: 

2) Of the statistically significant predictors you identified in the above question, which predictor coefficients (if any) indicated that a home is more likely to have a potentially dangerous level of lead in its drinking water as the value of the predictor/s increase? Which ones (if any) indicate that a home is less likely to have a potentially dangerous level of lead in its drinking water as the value of the predictor/s decrease? 

More likely as values of predictor/s increase/s: 

Your answer here:

Less likely as values of predictor/s increase/s:

Your answer here: 
